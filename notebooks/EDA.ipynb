{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31b2462d",
      "metadata": {},
      "source": [
        "# ODI Complaints EDA\n",
        "\n",
        "This notebook will be for our exploration and first cleaning steps of the complaint dataset. Other notebooks will be used to keep things like different models separate.\n",
        "\n",
        "The `src/` folder will be where the finalized workflow for each step is done so a main file can run them all in order and keep everything clean and reproducible. The files there won't be setup for exploring and getting outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579aed7c",
      "metadata": {},
      "source": [
        "## Quick Setup\n",
        "\n",
        "1. Run cells from top to bottom, first few cells setup the pathing\n",
        "2. The notebook loads the combined processed parquet by default\n",
        "3. A DataFrame named `df` is created for the combined with options for the separate datasets\n",
        "\n",
        "If the file is missing, run the pipeline first:\n",
        "- Windows: `./scripts/run_pipeline_windows.ps1`\n",
        "- macOS/Linux: `./scripts/run_pipeline_mac_linux.sh`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95cc132",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 220)\n",
        "pd.set_option(\"display.max_colwidth\", 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc4cd6a",
      "metadata": {},
      "source": [
        "Load the combined processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569901f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This notebook assumes the project setup matches the repo defaults\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if not (PROJECT_ROOT / \"data\" / \"processed\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "COMBINED_PATH = PROCESSED_DIR / \"odi_complaints_combined.parquet\"\n",
        "\n",
        "if not COMBINED_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find {COMBINED_PATH}. Run the pipeline first to create processed outputs.\"\n",
        "    )\n",
        "\n",
        "df = pd.read_parquet(COMBINED_PATH)\n",
        "df = df.drop(columns=[\"source_zip\", \"source_file\"], errors=\"ignore\")\n",
        "\n",
        "# Uncomment the following lines to load the individual year datasets if you want to compare them separately.\n",
        "# OLD_PATH = PROCESSED_DIR / \"COMPLAINTS_RECEIVED_2020-2024_processed.parquet\"\n",
        "# NEW_PATH = PROCESSED_DIR / \"COMPLAINTS_RECEIVED_2025-2026_processed.parquet\"\n",
        "# df_2020_2024 = pd.read_parquet(OLD_PATH)\n",
        "# df_2025_2026 = pd.read_parquet(NEW_PATH)\n",
        "\n",
        "print(\"Loaded:\", COMBINED_PATH.name)\n",
        "print(\"Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb7030f",
      "metadata": {},
      "source": [
        "## Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7e2fda",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Column names (first 25)\")\n",
        "display(pd.Series(df.columns, name=\"column\").head(25).to_frame())\n",
        "\n",
        "print(\"Data types (first 25)\")\n",
        "display(df.dtypes.rename(\"dtype\").reset_index(name=\"column\").head(25))\n",
        "\n",
        "print(\"First 5 rows\")\n",
        "display(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb419930",
      "metadata": {},
      "source": [
        "Get a quick count and percentage of null values by column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5fde6f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Null summary (overall)\n",
        "null_summary = (\n",
        "    df.isna()\n",
        "      .sum()\n",
        "      .sort_values(ascending=False)\n",
        "      .rename(\"null_count\")\n",
        "      .to_frame()\n",
        ")\n",
        "null_summary[\"null_pct\"] = (null_summary[\"null_count\"] / len(df) * 100).round(2)\n",
        "\n",
        "display(null_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c0cb32",
      "metadata": {},
      "source": [
        "There are a lot of null values, but some of it is because many columns are fields for a specific product type. So if 25% of the products are Type A and Type A products have five fields specific to it, then they'd have a minimum null percentage of 75%. This checks the null percentages based on the product type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f68ad421",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the distribution of rows by product type\n",
        "df[\"prod_type\"] = df[\"prod_type\"].astype(\"string\").fillna(\"<NA>\")\n",
        "\n",
        "prod_type_counts = (\n",
        "    df[\"prod_type\"].value_counts(dropna=False)\n",
        "    .reset_index(name=\"row_count\")\n",
        ")\n",
        "prod_type_counts[\"row_pct\"] = (prod_type_counts[\"row_count\"] / len(df) * 100).round(2)\n",
        "\n",
        "print(\"Rows by product type\")\n",
        "display(prod_type_counts)\n",
        "\n",
        "\n",
        "# Analyze null percentages by product type for columns with >5% nulls\n",
        "null_by_prod = pd.DataFrame({\"overall_null_pct\": (df.isna().mean() * 100).round(2)})\n",
        "\n",
        "for prod_value in prod_type_counts[\"prod_type\"].tolist():\n",
        "    mask = df[\"prod_type\"] == prod_value\n",
        "    null_by_prod[f\"null_pct_{prod_value}\"] = (df.loc[mask].isna().mean() * 100).round(2)\n",
        "    null_by_prod[f\"non_null_pct_{prod_value}\"] = (df.loc[mask].notna().mean() * 100).round(2)\n",
        "\n",
        "null_by_prod = null_by_prod.sort_values(\"overall_null_pct\", ascending=False)\n",
        "null_by_prod = null_by_prod[null_by_prod[\"overall_null_pct\"] > 5]\n",
        "\n",
        "print(\"Null by product type (more than 5% overall null)\")\n",
        "display(null_by_prod)\n",
        "\n",
        "\n",
        "# Identify columns that are potentially sparse\n",
        "print(\"Potentially sparse columns (overall null >= 80% and at least one product type with non-null >= 20%)\")\n",
        "sparse_candidates = null_by_prod[\n",
        "    (null_by_prod[\"overall_null_pct\"] >= 80)\n",
        "    & (null_by_prod.filter(regex=r\"^non_null_pct_\").max(axis=1) >= 20)\n",
        "]\n",
        "display(sparse_candidates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597a03c9",
      "metadata": {},
      "source": [
        "## Scratchpad "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2844c045",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vehicle-only view (likely the main analysis cohort)\n",
        "df_vehicle = df[df[\"prod_type\"] == \"V\"].copy()\n",
        "\n",
        "print(\"Vehicle-only rows:\", len(df_vehicle))\n",
        "\n",
        "vehicle_null_summary = (\n",
        "    df_vehicle.isna()\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .rename(\"null_count\")\n",
        "    .to_frame()\n",
        ")\n",
        "vehicle_null_summary[\"null_pct\"] = (vehicle_null_summary[\"null_count\"] / len(df_vehicle) * 100).round(2)\n",
        "\n",
        "display(vehicle_null_summary.head(20))\n",
        "\n",
        "\n",
        "# display(df_vehicle[[\"maketxt\", \"modeltxt\", \"compdesc\"]].head(20))\n",
        "# display(df_vehicle.groupby(\"compdesc\").size().sort_values(ascending=False).head(30))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}